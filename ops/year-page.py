from playwright.sync_api import sync_playwright
import pandas as pd
import time

def scrape_profit_table():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context()
        page = context.new_page()

        url = "https://www.cninfo.com.cn/new/disclosure/stock?orgId=gssz0000596&stockCode=000596#financialStatements"
        print(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
        page.goto(url, timeout=60000)
        page.wait_for_selector("div.financial-statements", timeout=20000)

        all_dfs = []
        page_num = 1

        while True:
            print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ...")

            page.wait_for_selector("div.financial-statements .el-table__header-wrapper", timeout=20000)

            # âœ… è·å–åˆ—å¤´ï¼ˆåªå–ç¬¬ä¸€è¡Œï¼‰
            header_rows = page.query_selector_all("div.financial-statements .el-table__header-wrapper thead tr")
            header_cells = header_rows[0].query_selector_all("th .cell")
            date_columns = [cell.inner_text().strip() for cell in header_cells if cell.inner_text().strip()]
            columns = ['æŒ‡æ ‡åç§°'] + date_columns
            print(f"âœ… åˆ—å¤´: {columns}")

            # âœ… æŠ“å–æ•°æ®è¡Œ
            rows = page.query_selector_all("div.financial-statements .el-table__body tbody tr")
            print(f"âœ… æ•°æ®è¡Œæ•°: {len(rows)}")
            data = []
            for idx, row in enumerate(rows):
                cells = row.query_selector_all("td .cell")
                row_data = [cell.inner_text().strip() for cell in cells]
                if len(row_data) == len(columns):
                    data.append(row_data)
                else:
                    print(f"âš ï¸ è·³è¿‡ç¬¬ {idx+1} è¡Œï¼ˆåˆ—æ•°ä¸åŒ¹é… {len(row_data)} vs {len(columns)}ï¼‰: {row_data}")

            df = pd.DataFrame(data, columns=columns)

            # âœ… ç­›é€‰â€œæŒ‡æ ‡åˆ—â€ + å¹´æŠ¥åˆ—ï¼ˆ12-31ï¼‰
            year_end_cols = [col for col in columns[1:] if col.endswith('12-31')]
            if year_end_cols:
                final_df = df[['æŒ‡æ ‡åç§°'] + year_end_cols]
                all_dfs.append(final_df)
                print(f"âœ… å½“å‰é¡µä¿ç•™ {len(year_end_cols)} å¹´æŠ¥åˆ—: {year_end_cols}")
            else:
                print("âš ï¸ å½“å‰é¡µæ— å¹´æŠ¥åˆ—ï¼Œè·³è¿‡")

            # âœ… åˆ¤æ–­æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
            next_btn = page.query_selector("button.btn-next:not([disabled])")
            if next_btn:
                next_btn.click()
                time.sleep(2)
                page_num += 1
            else:
                print("\nğŸš© æ²¡æœ‰ä¸‹ä¸€é¡µäº†ï¼Œç»“æŸç¿»é¡µ")
                break

        browser.close()

        # âœ… åˆå¹¶æ‰€æœ‰é¡µçš„ç»“æœï¼Œå¹¶å»é‡åˆ—
        if all_dfs:
            combined_df = pd.concat(all_dfs, axis=1)
            combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]
            print(f"\nâœ… æ‰€æœ‰å¹´æŠ¥æ•°æ®åˆå¹¶å®Œæˆï¼ˆ{combined_df.shape[0]} è¡Œ Ã— {combined_df.shape[1]} åˆ—ï¼‰:")
            print(combined_df)
        else:
            print("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•å¹´æŠ¥æ•°æ®")

if __name__ == "__main__":
    scrape_profit_table()
